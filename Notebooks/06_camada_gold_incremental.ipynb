{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f6ebb2-bd1f-472b-9df9-3315acf55566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Carga Incremental na Camada Gold (Delta)\n",
    "\n",
    "O código apresentado é parte do processo de carga incremental de dados na Camada Gold da arquitetura Medallion. Essa camada é responsável por armazenar dados otimizados para consumo em análises, com tabelas de fatos e dimensões organizadas e enriquecidas.\n",
    "\n",
    "No exemplo, a configuração da SparkSession e os caminhos para as camadas Silver e Gold no Data Lake são definidos, preparando o ambiente para leitura e gravação de dados no formato Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6398c78-3c20-4556-879c-1a7d68216bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importação da biblioteca necessária para criar a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuração da SparkSession para suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Carga Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definindo os caminhos para as camadas no Data Lake\n",
    "silver_path = \"/mnt/lhdw/silver/vendas\"  # Localização dos dados refinados da camada Silver\n",
    "gold_path = \"/mnt/lhdw/gold/vendas_delta\"  # Localização para salvar as dimensões na camada Gold\n",
    "gold_fato_path = \"/mnt/lhdw/gold/vendas_delta/fato_vendas\"  # Localização específica para tabela fato\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e123f4-c2b6-4147-ad69-54a5a21c91d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Leitura Incremental de Dados na Camada Silver\n",
    "\n",
    "O código realiza a leitura incremental da Camada Silver com base na maior data já registrada na tabela Fato Vendas na Camada Gold. Essa abordagem é essencial para garantir eficiência e evitar reprocessamento de dados já consumidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed422817-9bbb-4677-9cec-460924d1fe2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.date(2012, 12, 31)Out[6]: 0"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, lit  # Importação de funções auxiliares\n",
    "\n",
    "# Ler a maior data de venda da tabela Fato Vendas na Camada Gold\n",
    "# selectExpr(\"max(DataVenda) as MaxDataVenda\"): Obtém o valor máximo da coluna DataVenda\n",
    "# collect()[0][\"MaxDataVenda\"]: Recupera o valor do registro coletado para uso no filtro\n",
    "max_data_venda = spark.read.format(\"delta\").load(gold_fato_path) \\\n",
    "    .selectExpr(\"max(DataVenda) as MaxDataVenda\") \\\n",
    "    .collect()[0][\"MaxDataVenda\"]\n",
    "\n",
    "# Exibir a maior data obtida\n",
    "display(max_data_venda)\n",
    "\n",
    "# Carregar os dados da Camada Silver filtrando apenas os registros mais recentes que a DataVenda obtida\n",
    "df_silver = spark.read.format(\"parquet\").load(silver_path) \\\n",
    "    .filter(f\"Data > '{max_data_venda}'\")  # Filtra os dados cuja Data é maior que a maior DataVenda\n",
    "\n",
    "# Contar o número de registros no DataFrame carregado\n",
    "df_silver.count()\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae2901bb-159e-4c49-a35a-5794d4147069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Produto com Atualização Temporal\n",
    "\n",
    "O código cria a tabela de dimensão Produto na Camada Gold, atribuindo chaves substitutas (surrogate keys) e adicionando uma coluna para rastrear a data de atualização dos registros. Isso é essencial para garantir a integridade das dimensões e habilitar análises temporais e incrementais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd61be0-b1b8-4087-bbce-48a15691d94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>IDProduto</th><th>Produto</th><th>Categoria</th><th>sk_produto</th><th>data_atualizacao</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "IDProduto",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Produto",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Categoria",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sk_produto",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "data_atualizacao",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp  # Importação de funções necessárias\n",
    "\n",
    "# Nome da tabela de destino\n",
    "tb_destino = \"dim_produto\"\n",
    "\n",
    "# Passo 1 - Extrair produtos únicos para a dimensão Produto\n",
    "dim_produto_df = df_silver.select(\n",
    "    \"IDProduto\", \"Produto\", \"Categoria\"  # Seleção das colunas relevantes\n",
    ").dropDuplicates()  # Remover duplicados para garantir unicidade\n",
    "\n",
    "# Passo 2 - Adicionar chave substituta (surrogate keys) e data de atualização\n",
    "dim_produto_df = dim_produto_df.withColumn(\"sk_produto\", monotonically_increasing_id() + 1) \\\n",
    "                               .withColumn(\"data_atualizacao\", current_timestamp())  # Adicionar timestamp atual\n",
    "\n",
    "# Passo 3 - Escrever a Dimensão Produto no formato Delta\n",
    "# option(\"mergeSchema\", \"true\"): Garantir compatibilidade de esquema\n",
    "dim_produto_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(f\"{gold_path}/{tb_destino}\")\n",
    "\n",
    "# Passo 4 - Visualizar a tabela gerada\n",
    "display(dim_produto_df)  # Exibe os dados carregados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba96d39-f6e9-4971-8b77-3c9475761626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Categoria com Atualização Temporal\n",
    "\n",
    "O código constrói a tabela de dimensão Categoria na Camada Gold, utilizando chaves substitutas (surrogate keys) e registrando a data de atualização para garantir a rastreabilidade dos dados. Essa dimensão é fundamental em modelos dimensionais para organizar categorias únicas associadas a outros dados, como vendas ou produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76509140-b502-48ba-9f5e-24844a45ea34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp  # Importação de funções necessárias\n",
    "\n",
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"dim_categoria\"\n",
    "\n",
    "# Passo 1 - Extrair categorias únicas para a dimensão\n",
    "dim_categoria_df = df_silver.select(\n",
    "    \"Categoria\"  # Seleciona apenas a coluna relevante\n",
    ").dropDuplicates()  # Remove duplicatas para garantir unicidade das categorias\n",
    "\n",
    "# Passo 2 - Adicionar chave substituta (surrogate keys) e data de atualização\n",
    "dim_categoria_df = dim_categoria_df.withColumn(\"sk_categoria\", monotonically_increasing_id() + 1) \\\n",
    "                                   .withColumn(\"data_atualizacao\", current_timestamp())  # Adiciona o timestamp atual\n",
    "\n",
    "# Passo 3 - Escrever a Dimensão Categoria no formato Delta\n",
    "dim_categoria_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(f\"{gold_path}/{tb_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1842462d-3602-45ab-889b-55c8e53d24c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Segmento com Atualização Temporal\n",
    "\n",
    "Este código constrói a tabela de dimensão Segmento na Camada Gold, utilizando chaves substitutas (surrogate keys) e adicionando uma coluna de controle com a data de atualização. A dimensão Segmento é usada em modelos analíticos para categorizar ou agrupar dados transacionais, permitindo análises mais detalhadas e organizadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5140696a-4139-4159-8a3a-055d38ece6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp  # Importação das funções necessárias\n",
    "\n",
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"dim_segmento\"\n",
    "\n",
    "# Passo 1 - Extrair segmentos únicos para a dimensão\n",
    "dim_segmento_df = df_silver.select(\n",
    "    \"Segmento\"  # Seleciona apenas a coluna Segmento\n",
    ").dropDuplicates()  # Remove duplicatas, garantindo unicidade\n",
    "\n",
    "# Passo 2 - Adicionar chave substituta (surrogate key) e data de atualização\n",
    "dim_segmento_df = dim_segmento_df.withColumn(\"sk_segmento\", monotonically_increasing_id() + 1) \\\n",
    "                                 .withColumn(\"data_atualizacao\", current_timestamp())  # Adiciona timestamp atual\n",
    "\n",
    "# Passo 3 - Escrever a Dimensão Segmento no formato Delta\n",
    "dim_segmento_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(f\"{gold_path}/{tb_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8cf69d-e3fa-45f9-a2da-c9c07258e526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Fabricante com Atualização Temporal\n",
    "\n",
    "Explicação: Criação da Dimensão Fabricante com Atualização Temporal\n",
    "Este código cria a tabela de dimensão Fabricante na Camada Gold do Data Lake. Ele utiliza chaves substitutas (surrogate keys) para garantir um identificador único por fabricante e adiciona uma coluna de controle para rastrear a data de atualização. A dimensão Fabricante é essencial para identificar e categorizar produtos por seus fabricantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd8ae90-4dba-4cd1-a7e2-8ba19cd49657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp  # Importação de funções para manipulação de dados\n",
    "\n",
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"dim_fabricante\"\n",
    "\n",
    "# Passo 1 - Extrair fabricantes únicos para a dimensão Fabricante\n",
    "dim_fabricante_df = df_silver.select(\n",
    "    \"IDFabricante\",  # Identificador único do fabricante\n",
    "    \"Fabricante\"  # Nome do fabricante\n",
    ").dropDuplicates()  # Remove duplicatas para garantir unicidade\n",
    "\n",
    "# Passo 2 - Adicionar chave substituta e coluna de controle\n",
    "dim_fabricante_df = dim_fabricante_df.withColumn(\"sk_fabricante\", monotonically_increasing_id() + 1) \\\n",
    "                                      .withColumn(\"data_atualizacao\", current_timestamp())  # Adiciona a data de atualização atual\n",
    "\n",
    "# Passo 3 - Escrever a tabela DimFabricante no formato Delta\n",
    "dim_fabricante_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(f\"{gold_path}/{tb_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5c9c48-3c05-4cfa-b858-0d98b843a440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Geografia\n",
    "\n",
    "Este código cria a dimensão Geografia na camada Gold, consolidando informações relacionadas a localidades únicas, como cidade, estado e país. A dimensão Geografia é fundamental para análises geográficas e relatórios que envolvem distribuição regional de vendas, clientes ou produtos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9cb6c3f-6543-441f-a304-c86ffb60ef5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp  # Importação de funções necessárias\n",
    "\n",
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"dim_geografia\"\n",
    "\n",
    "# Passo 1 - Extrair informações geográficas únicas para a dimensão Geografia\n",
    "dim_geografia_df = df_silver.select(\n",
    "    \"Cidade\",       # Nome da cidade\n",
    "    \"Estado\",       # Nome do estado\n",
    "    \"Regiao\",       # Região associada\n",
    "    \"Distrito\",     # Distrito administrativo\n",
    "    \"Pais\",         # Nome do país\n",
    "    \"CodigoPostal\"  # Código postal\n",
    ").dropDuplicates()  # Remove duplicatas para garantir unicidade\n",
    "\n",
    "# Passo 2 - Adicionar chave substituta (SK) e coluna de controle\n",
    "dim_geografia_df = dim_geografia_df.withColumn(\"sk_geografia\", monotonically_increasing_id() + 1) \\\n",
    "                                   .withColumn(\"data_atualizacao\", current_timestamp())  # Data de atualização atual\n",
    "\n",
    "# Passo 3 - Escrever a tabela DimGeografia no formato Delta\n",
    "dim_geografia_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(f\"{gold_path}/{tb_destino}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e39d75-c09d-4088-ac72-f5fe5a9332d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Cliente\n",
    "\n",
    "O código cria a tabela de dimensão Cliente na Camada Gold do Data Lake. Essa tabela relaciona os clientes com a geografia associada por meio de chaves substitutas (surrogate keys), facilitando análises futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf439c04-ea5c-421c-9c76-e51123c47a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"dim_cliente\"\n",
    "\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, current_timestamp\n",
    "\n",
    "# Passo 1: Selecionar dados únicos para os clientes\n",
    "dim_cliente_df = df_silver.select(\n",
    "    \"IDCliente\",     # Identificador único do cliente\n",
    "    \"Nome\",          # Nome do cliente\n",
    "    \"Email\",         # E-mail do cliente\n",
    "    \"Cidade\",        # Cidade associada ao cliente\n",
    "    \"Estado\",        # Estado do cliente\n",
    "    \"Regiao\",        # Região (ex.: Norte, Sul)\n",
    "    \"Distrito\",      # Distrito do cliente\n",
    "    \"Pais\",          # País do cliente\n",
    "    \"CodigoPostal\"   # CEP do cliente\n",
    ").dropDuplicates()   # Remove duplicatas para garantir unicidade\n",
    "\n",
    "# Passo 2: Associar SK_Geografia com base nos dados geográficos\n",
    "dim_cliente_com_sk_df = dim_cliente_df.alias(\"cliente\") \\\n",
    "    .join(dim_geografia_df.alias(\"geografia\"), \n",
    "          (col(\"cliente.Cidade\") == col(\"geografia.Cidade\")) &           # Combina cidade\n",
    "          (col(\"cliente.Estado\") == col(\"geografia.Estado\")) &           # Combina estado\n",
    "          (col(\"cliente.Regiao\") == col(\"geografia.Regiao\")) &           # Combina região\n",
    "          (col(\"cliente.Distrito\") == col(\"geografia.Distrito\")) &       # Combina distrito\n",
    "          (col(\"cliente.Pais\") == col(\"geografia.Pais\")) &               # Combina país\n",
    "          (col(\"cliente.CodigoPostal\") == col(\"geografia.CodigoPostal\")), # Combina CEP\n",
    "          \"left\") \\\n",
    "    .select(\"cliente.IDCliente\", \"cliente.Nome\", \"cliente.Email\", \"geografia.sk_geografia\")  # Seleciona colunas necessárias\n",
    "\n",
    "# Passo 3: Adicionar chave substituta e coluna de auditoria\n",
    "dim_cliente_com_sk_df = dim_cliente_com_sk_df.withColumn(\"sk_cliente\", monotonically_increasing_id()+1) \\\n",
    "                                             .withColumn(\"data_atualizacao\", current_timestamp())  # Data e hora da atualização\n",
    "\n",
    "# Passo 4: Selecionar colunas finais\n",
    "dim_cliente_com_sk_df = dim_cliente_com_sk_df.select(\n",
    "    \"IDCliente\",       # Identificador original do cliente\n",
    "    \"Nome\",            # Nome do cliente\n",
    "    \"Email\",           # E-mail do cliente\n",
    "    \"sk_geografia\",    # Chave substituta da geografia\n",
    "    \"sk_cliente\",      # Chave substituta do cliente\n",
    "    \"data_atualizacao\" # Data e hora da última atualização\n",
    ")\n",
    "\n",
    "# Passo 5: Escrever a tabela no formato Delta\n",
    "dim_cliente_com_sk_df.write.format(\"delta\") \\\n",
    "                           .mode(\"overwrite\") \\\n",
    "                           .option(\"mergeSchema\", \"true\") \\\n",
    "                           .save(f\"{gold_path}/{tb_destino}\")  # Caminho da tabela na Camada Gold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1894f40d-60d7-46e9-bd54-69fe71351f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação de Tabela Fato\n",
    "\n",
    "Este código cria a tabela fato_vendas na Camada Gold do Data Lake. A tabela fato consolida os dados transacionais, associando-os às tabelas de dimensões por meio de chaves substitutas (SKs) e particiona os dados para otimizar consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b8b3eb-b651-4f33-a31c-7003a1285f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nome da tabela de destino no Delta Lake\n",
    "tb_destino = \"fato_vendas\"\n",
    "\n",
    "from pyspark.sql.functions import broadcast, year, month, current_timestamp\n",
    "\n",
    "# Passo 1: Juntar dados da Silver com as tabelas de dimensões\n",
    "fato_vendas_df = df_silver.alias(\"s\") \\\n",
    "    .join(broadcast(dim_produto_df.select(\"IDProduto\", \"sk_produto\").alias(\"dprod\")), \"IDProduto\") \\\n",
    "    .join(broadcast(dim_categoria_df.select(\"Categoria\", \"sk_categoria\").alias(\"dcat\")), \"Categoria\") \\\n",
    "    .join(broadcast(dim_segmento_df.select(\"Segmento\", \"sk_segmento\").alias(\"dseg\")), \"Segmento\") \\\n",
    "    .join(broadcast(dim_fabricante_df.select(\"Fabricante\", \"sk_fabricante\").alias(\"dfab\")), \"Fabricante\") \\\n",
    "    .join(broadcast(dim_cliente_com_sk_df.select(\"IDCliente\", \"sk_cliente\").alias(\"dcli\")), \"IDCliente\") \\\n",
    "    .select(\n",
    "        col(\"s.Data\").alias(\"DataVenda\"),  # Data da venda\n",
    "        \"sk_produto\",                     # Chave substituta do produto\n",
    "        \"sk_categoria\",                   # Chave substituta da categoria\n",
    "        \"sk_segmento\",                    # Chave substituta do segmento\n",
    "        \"sk_fabricante\",                  # Chave substituta do fabricante\n",
    "        \"sk_cliente\",                     # Chave substituta do cliente\n",
    "        \"Unidades\",                       # Quantidade de unidades vendidas\n",
    "        col(\"s.PrecoUnitario\"),           # Preço unitário do produto\n",
    "        col(\"s.CustoUnitario\"),           # Custo unitário do produto\n",
    "        col(\"s.TotalVendas\"),             # Total da venda\n",
    "        current_timestamp().alias(\"data_atualizacao\")  # Data e hora da atualização\n",
    "    )\n",
    "\n",
    "# Passo 2: Particionar dados por ano e mês da DataVenda\n",
    "fato_vendas_df = fato_vendas_df.withColumn(\"Ano\", year(\"DataVenda\")) \\\n",
    "                               .withColumn(\"Mes\", month(\"DataVenda\"))\n",
    "\n",
    "# Passo 3: Escrever a tabela Fato no formato Delta, particionada por Ano e Mês\n",
    "# mode(\"append\"): Modo incremental para adicionar novos dados\n",
    "fato_vendas_df.write.format(\"delta\") \\\n",
    "             .mode(\"append\") \\\n",
    "             .option(\"mergeSchema\", \"true\") \\\n",
    "             .option(\"MaxRecordsPerFile\", 1000000) \\\n",
    "             .partitionBy(\"Ano\", \"Mes\") \\\n",
    "             .save(f\"{gold_path}/{tb_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934e4510-bdda-4d01-805d-a3ab49c63b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demonstração de informação total vendas por ano\n",
    "\n",
    "O código fornecido realiza uma consulta na tabela fato_vendas, agrupando os dados por ano e somando o valor total das vendas para cada ano. Além disso, a consulta ordena os resultados de forma decrescente com base no valor total de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b06b72-290a-495b-a785-3389aa5f86f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Ano</th><th>SomaTotalVendas</th></tr></thead><tbody><tr><td>2012</td><td>1.1399112610003723E7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2012,
         1.1399112610003723E7
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Ano",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "SomaTotalVendas",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Define o caminho onde os dados da camada Gold estão armazenados\n",
    "gold_path = \"/mnt/lhdw/gold/vendas_delta/\"\n",
    "\n",
    "# Lê a tabela 'fato_vendas' no formato Delta\n",
    "# A função 'read.format(\"delta\")' permite que o Spark leia os dados do formato Delta\n",
    "# Agrupa os dados pela coluna 'Ano' para calcular a soma das vendas por ano\n",
    "# A função 'agg' permite realizar agregações, neste caso somar os valores da coluna 'TotalVendas'\n",
    "# Ordena os resultados pela coluna 'Ano' e pela soma total das vendas em ordem decrescente\n",
    "resultado = spark.read.format(\"delta\").load(f\"{gold_path}/fato_vendas\") \\\n",
    "    .groupBy(\"Ano\") \\\n",
    "    .agg(sum(\"TotalVendas\").alias(\"SomaTotalVendas\")) \\\n",
    "    .orderBy(col(\"Ano\"), col(\"SomaTotalVendas\").desc())\n",
    "\n",
    "# Exibe o resultado da consulta (somatório das vendas por ano)\n",
    "display(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "907ddce9-46b7-43d6-9975-e831e1e0f4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47ecd5e-a69c-490b-9dc0-52c5ef5762aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Limpeza de Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27462067-a7da-4f34-b982-e310dc50e3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 668"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Coletar lixo após operações pesadas para liberar memória\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_camada_gold_incremental",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
