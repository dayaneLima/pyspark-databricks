# Notebooks para Databricks

Este repositório reúne notebooks projetados para o ambiente **Databricks**, com foco no uso de **Apache Spark** e **PySpark** para processamento e análise de dados.

## Conteúdo

Os notebooks incluem os seguintes temas:

- **Introdução ao Apache Spark e PySpark**: Conceitos básicos e principais funcionalidades para o processamento distribuído.
- **Configuração do Databricks**: Guia prático para configurar e usar o ambiente Databricks de forma eficiente.
- **Gerenciamento de tabelas Delta**: Criação, atualização e manutenção de tabelas transacionais com Delta Lake.
- **Manipulação de dados com PySpark**: Transformações, operações e técnicas de engenharia de dados.
- **Pipelines de dados**: Implementação de fluxos automatizados para integração e processamento de dados.
- **Análise de dados**: Métodos e ferramentas para exploração de grandes volumes de dados.

## Requisitos

Para executar os notebooks deste repositório, você precisará de:

- Acesso ao [Databricks](https://databricks.com/).
- Um cluster configurado com **Apache Spark**.
- Noções de programação em Python e Spark.

## Como usar
Siga as etapas abaixo para importar e configurar os notebooks deste repositório no ambiente Databricks:

1. Clone este repositório:
   ```bash
   git clone https://github.com/dayaneLima/pyspark-databricks.git
   ```
   
2. **Acesse seu workspace no Databricks**  
   Faça login em sua conta do Databricks e navegue até seu workspace.

3. **Use a opção para importar arquivos de notebooks**  
   - No menu do workspace, clique em **Workspace** ou **Repos**.  
   - Escolha a opção **Import** (geralmente disponível no botão com ícone de "Importar").  

4. **Faça o upload dos notebooks deste repositório**  
   - Selecione os arquivos de notebook baixados do repositório local em seu computador.  
   - Confirme para que os notebooks sejam adicionados ao Databricks.

5. **Configure as conexões ou variáveis necessárias**  
   - Abra cada notebook e configure as variáveis e credenciais, como chaves de acesso ou caminhos de armazenamento, conforme indicado nos comentários.

6. **Execute os notebooks**  
   - Execute cada célula do notebook seguindo as instruções fornecidas nos comentários para garantir o funcionamento correto.

---

Após importar, seus notebooks estarão prontos para uso no Databricks!

